{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caabfcdf",
   "metadata": {},
   "source": [
    "# Demo\n",
    "\n",
    "This jupyter notebook contains a demo / introduction to using our implementation of Stochastic Gradient Hamiltionian Monte Carlo (SGHMC) which is built on top of Pyro. There are a few caveats that you should understand before trying to use our implementation, so this idea of this document is to help you to use our implementation correctly.\n",
    "\n",
    "This demo is very loosely inspired by: https://bookdown.org/robertness/causalml/docs/tutorial-on-deep-probabilitic-modeling-with-pyro.html#introduction-to-pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e062331",
   "metadata": {},
   "source": [
    "# Pyro\n",
    "\n",
    "Pyro is a universal probabilistic programming language (PPL) written in Python and supported by PyTorch on the backend. Pyro enables flexible and expressive deep probabilistic modeling, unifying the best of modern deep learning and Bayesian modeling.\n",
    "\n",
    "What this really means: Pyro provides a vast array of modelling and inference abilities, which are super-charged by PyTorch's automatic differentiation and GPU computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d26bc2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import pyro\n",
    "import numpy as np\n",
    "from pyro.infer.mcmc import MCMC\n",
    "import pyro.distributions as dist\n",
    "# our implementation of sghmc\n",
    "from kernel.sghmc import SGHMC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82f483b",
   "metadata": {},
   "source": [
    "# 1) Defining a model (aka, stochastic function)\n",
    "\n",
    "In Pyro, a model is defined using a **\"stochastic function\"**.\n",
    "\n",
    "A stochastic function is an arbitrary Python callable (function or method) that combines two ingredients:\n",
    "\n",
    "- Deterministic Python code\n",
    "- Primitive stochastic functions that call a random number generator.\n",
    "\n",
    "## Primitive stochastic functions\n",
    "\n",
    "These are basic elements which allow us to introduce stochasticity into our model, and essentially allow us to define different **distributions** in our model. The code below defines a normal distribution and samples from it:\n",
    "\n",
    "## Adding data to the model\n",
    "\n",
    "The key idea of SGHMC is that we subsample the observed data explained by the stochastic model and compute noisy gradient estimates which are used to simulate Hamiltonian dynamics in the same way as noise free HMC.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba4bec0",
   "metadata": {},
   "source": [
    "### A Simple model:\n",
    "\n",
    "This model is flexible it can take an arbitrary amount of data and condition on it to give us a posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6e580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    weight = pyro.sample(\"weight\", dist.Normal(0, 1.0))\n",
    "    return pyro.sample(\"measurement\", dist.Normal(weight, 1.0), obs=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8192772",
   "metadata": {},
   "source": [
    "### Some data\n",
    "Below we create some data for our model to condition on. Note that any data passed to the model to be subsampled **must** first be wrapped in a pytorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0190ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor([8.5992, 6.4611, 6.7287, 4.2451, 5.5909, 5.4184, 7.3571, 6.3754, 8.0597,\n",
    "        8.7034])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0959e7e9",
   "metadata": {},
   "source": [
    "# 2) Inference\n",
    "\n",
    "## Inference using MCMC\n",
    "\n",
    "Now, all we need to do is use an inference algorithm to estimate the posterior distribution of the unobserved random variables in our model (in this case, the weight of the object), given the observed data.\n",
    "\n",
    "### Using SGHMC\n",
    "\n",
    "Our data is positional argument 0 of our model, this is the default behaviour, although if we want to subsample more than one positional arguments then we need to specify the positions in the `subsample_positions` argument.\n",
    "\n",
    "We can specify other things such as `batch_size`, `learning_rate`, `momentum_decay`, `num_steps`, `obs_info_noise`, `compute_obs_info`\n",
    "\n",
    "Ideally we want the `learning_rate` to be low, the smaller the learning rate the closer the samples are to the true posterior, this is a trade off though because with a higher learning rate we will explore more of the posterior mass more quickly. the `momentum_decay` is also important to tune and should be set in the range `0.001 - 0.1`. `obs_info_noise` is a useful tool for estimating the gradient noise and improves sampling but it will likely only work for medium to low dimensional models.\n",
    "\n",
    "**deprecated** *see kernel.legacy.sghmc:*\n",
    "\n",
    "*Note that the naive SGHMC algorithm does **not** use friction (but we can turn Metropolis Hastings correction on for this configuration and see the difference in performance)*\n",
    "\n",
    "*Also note for the SGHMC algorithm we do **not** use MH correction since by using friction we get the desired invariant distribution.*\n",
    "\n",
    "*Its best not to set `do_step_size_adaptation = True` at the moment since the automatic step size adaptation is very unstable at the moment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bca05cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|███████████████████████████████████████████████████████████████| 2000/2000 [00:34, 58.55it/s, lr=1.00e-02]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Stochastic Gradient Hamiltonian Monte Carlo kernel.\n",
    "\n",
    "    Implements SGHMC as described in [1]_. The basic algorithm is described in\n",
    "    Algorithm 2 there. The actual implementation uses the reparametrisation\n",
    "    described in the subsection 'CONNECTION TO SGD WITH MOMENTUM'.\n",
    "\n",
    "    The weight matrix is taken to be the identity.\n",
    "\n",
    "    The noise model is estimated using the observed information.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    model\n",
    "        The pyro model from which to sample.\n",
    "\n",
    "    subsample_positions : list, default [0] / 1st positional argument only\n",
    "        Specifies which positional arguments of the model to subsample during runtime\n",
    "        \n",
    "    batch_size : int, default=5\n",
    "        The size of the minibatches to use\n",
    "\n",
    "    learning_rate : float, default=0.1\n",
    "        The learning rate of the algorithm, used analogously to that in\n",
    "        stochastic gradient descent. This value is the same as the square of\n",
    "        the step size parameter used in discretely simulating the Hamiltonian \n",
    "        dynamics.\n",
    "\n",
    "    momentum_decay : float, default=0.01\n",
    "        The momentum decay rate, used analogously to that in stochastic \n",
    "        gradient descent with momentum. The friction term `C` is related to \n",
    "        this as:\n",
    "            momentum_decay = sqrt(momentum_decay) * I C\n",
    "        where I is the identity matrix.\n",
    "\n",
    "    num_steps : int, default 10\n",
    "        The number of steps to simulate Hamiltonian dynamics.\n",
    "\n",
    "    resample_every_n : int, default 50\n",
    "        When to resmaple to momentum (default is to resample momentum every 50 samples)\n",
    "\n",
    "    obs_info_noise : bool, default=False\n",
    "        Use the observed information to estimate the noise model\n",
    "\n",
    "    compute_obs_info : string, default=\"every_sample\", valid=[\"start\", \"every_sample\", \"every_step\"]\n",
    "        When to compute the observed information matrix to estimate B hat,\n",
    "        - \"start\" once at the beginning using the initial parameters\n",
    "        - \"every_sample\" once at the start of every sampling procedure\n",
    "        - \"every_step\" at every integration step or when the parameters change\n",
    "\n",
    "    device\n",
    "        The PyTorch device to use.\n",
    "\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Tianqi Chen, Emily B. Fox, Carlos Guestrin, \"Stochastic Gradient \n",
    "       Hamiltonian Monte Carlo\", arXiv:1402.4102 [stat.ME],  \t\n",
    "       https://doi.org/10.48550/arXiv.1402.4102\n",
    "    \"\"\"\n",
    "    \n",
    "# Instantiate the SGHMC kernel\n",
    "sghmc_kernel = SGHMC(model,\n",
    "                     subsample_positions=[0],\n",
    "                     batch_size=5,\n",
    "                     learning_rate=0.01,\n",
    "                     momentum_decay= 0.1,\n",
    "                     num_steps=10,\n",
    "                     resample_every_n=50,\n",
    "                     obs_info_noise=False,\n",
    "                     compute_obs_info='every_sample')\n",
    "\n",
    "# Use pyros MCMC sampling module\n",
    "sghmc_mcmc = MCMC(sghmc_kernel, num_samples=1000)\n",
    "# Pass the model parameters when we call run\n",
    "sghmc_mcmc.run(data)\n",
    "# Fetch samples from the sampler\n",
    "sghmc_samples = sghmc_mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f214d9d2",
   "metadata": {},
   "source": [
    "### Plotting the data\n",
    "We can plot the empirical posterior distribution by plotting `sghmc_samples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "332fec6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = sghmc_samples['weight'].numpy()\n",
    "\n",
    "plt.hist(samples, density=True, bins=30)  # density=False would make counts\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b817fac4",
   "metadata": {},
   "source": [
    "### Using SGLD\n",
    "\n",
    "Again, our data is positional argument 0 of our model, this is the default behaviour, although if we want to subsample more than one positional arguments then we need to specify the positions in the `subsample_positions` argument.\n",
    "\n",
    "We can specify other things such as `batch_size`, `learning_rate`, `noise_rate`, `num_steps`,`obs_info_noise`, `compute_obs_info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cac8621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████████████████████████████████████████████████████████| 2000/2000 [00:17, 117.40it/s, lr=1.00e-01]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Stochastic Gradient with Langevin Dynamics.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subsample_positions : list, default [0] / 1st positional argument only\n",
    "        Specifies which positional arguments of the model to subsample during runtime\n",
    "        \n",
    "    batch_size : int, default=5\n",
    "        The size of the minibatches to use\n",
    "\n",
    "    learning_rate : int, default 0.1\n",
    "        The size of a single step taken during sampling\n",
    "\n",
    "    num_steps : int, default 10\n",
    "        The number of steps to simulate Hamiltonian dynamics\n",
    "\n",
    "    obs_info_noise : bool, default=False\n",
    "        Use the observed information to estimate the noise model\n",
    "\n",
    "    compute_obs_info : string, default=None, valid=[\"start\", \"every_sample\", \"every_step\", None]\n",
    "        When to compute the observed information matrix to estimate B hat,\n",
    "        - \"start\" once at the begining using the inital parameters\n",
    "        - \"every_sample\" once at the start of every sampling procedure\n",
    "        - \"every_step\" at every intehration step or when the parameters change\n",
    "    \"\"\"\n",
    "\n",
    "from kernel.sgld import SGLD\n",
    "\n",
    "sgld_kernel = SGLD(model, \n",
    "                   subsample_positions=[0], \n",
    "                   batch_size=5, \n",
    "                   learning_rate=0.1, \n",
    "                   num_steps=5, \n",
    "                   obs_info_noise=False,\n",
    "                   compute_obs_info=None)\n",
    "\n",
    "sgld_mcmc = MCMC(sgld_kernel, num_samples=1000)\n",
    "sgld_mcmc.run(data)\n",
    "sgld_samples = sgld_mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8f3168",
   "metadata": {},
   "source": [
    "### Plotting the data\n",
    "We can plot the empirical posterior distribution output by SGLD by plotting `sgld_samples` and compare the two sampling algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c4120c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = sgld_samples['weight'].numpy()\n",
    "\n",
    "plt.hist(samples, density=True, bins=30)  # density=False would make counts\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f8c4ad",
   "metadata": {},
   "source": [
    "### Using SGD\n",
    "\n",
    "Our library has an implementation of Stochastic Gradient Descent which unintuitively subclasses pyros MCMCKernel (simply for convenience). Note that the samples drawn from SGD are not from the posterior since SGD is an optimization algorithm, so take extra care when dealing with the samples.\n",
    "\n",
    "As before our data is positional argument 0 of our model, this is the default behaviour, although if we want to subsample more than one positional arguments then we need to specify the positions in the `subsample_positions` argument.\n",
    "\n",
    "We can specify other things such as `batch_size`, `learning_rate`, `weight_decay`, `with_momentum`, `momentum_decay`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1c5ca27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████████████████████████████████████████████████████████| 1000/1000 [00:02, 382.85it/s, lr=1.00e-03]\n"
     ]
    }
   ],
   "source": [
    "from kernel.sgd import SGD\n",
    "\n",
    "sgd_kernel = SGD(model, \n",
    "                 subsample_positions=[0], \n",
    "                 batch_size=5, \n",
    "                 learning_rate=0.001, \n",
    "                 weight_decay=0.0,\n",
    "                 with_momentum=True,\n",
    "                 momentum_decay=0.01)\n",
    "\n",
    "sgd_mcmc = MCMC(sgd_kernel, num_samples=1000, warmup_steps=0) # warmup_steps is set to 0 so we can see the full SGD dynamics\n",
    "sgd_mcmc.run(data)\n",
    "sgd_samples = sgd_mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0c148",
   "metadata": {},
   "source": [
    "### Plotting the data\n",
    "We can plot the sample space that SGD traversed by plotting `sgd_samples`. We should see SGD honing in on the MAP estimate rather than sampling from the full posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1843db8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "samples = sgd_samples['weight'].numpy()\n",
    "\n",
    "plt.hist(samples, bins=30)  # density=False would make counts\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b98c3ba",
   "metadata": {},
   "source": [
    "## More flexible models\n",
    "\n",
    "Our implementation of SGHMC is flexible enough to allow you to specify arbitrary positional and keyword arguments. Although you can **only** subsample positional arguments!!! This shouldn't be too restrictive of a requirement, but below we give good some examples of how to use our implementation of SGHMC properly and some bad examples.\n",
    "\n",
    "We won't run any inference but if you're interested but the following examples will be based on coin flipping using a beta prior and binomial likelihood. See `example.ipynb` and play around with it to convince yourself the following examples will work or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e4afb6",
   "metadata": {},
   "source": [
    "### Model with arbitrary key word arguments\n",
    "\n",
    "In this example we supply our model with hyper-priors for the beta distribution by using key word arguments. The default is `alpha0=1.` and `beta0=1.` which is just the uniform distribution on the unit interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd14d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, alpha0=1., beta0=1.):\n",
    "    alpha0 = torch.tensor(alpha0)\n",
    "    beta0 = torch.tensor(beta0)\n",
    "    \n",
    "    f = pyro.sample(\"coin\", dist.Beta(alpha0, beta0))\n",
    "    \n",
    "    return pyro.sample(\"obs\", dist.Bernoulli(f), obs=dats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1dcdf9",
   "metadata": {},
   "source": [
    "If we want to run inference using our implementation of SGHMC we needn't change much we just need to specify `alpha0` and `beta0` by passing it to the `.run()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SGHMC kernel\n",
    "# Instantiate the SGHMC kernel\n",
    "sghmc_kernel = SGHMC(model,\n",
    "                     subsample_positions=[0],\n",
    "                     batch_size=5,\n",
    "                     learning_rate=0.01,\n",
    "                     momentum_decay= 0.1,\n",
    "                     num_steps=10,\n",
    "                     resample_every_n=50,\n",
    "                     obs_info_noise=False,\n",
    "                     compute_obs_info=None)\n",
    "\n",
    "# Use pyros MCMC sampling module\n",
    "sghmc_mcmc = MCMC(sghmc_kernel, num_samples=1000)\n",
    "# Pass the model parameters when we call run\n",
    "# Here we specify a stronger prior over the latent fairness of the coinflip\n",
    "sghmc_mcmc.run(data, alpha0=10., beta0=10.) # <------------------------\n",
    "# Alternatively we can specify neither alpha0 or beta0 and use their default values\n",
    "sghmc_mcmc.run(data)\n",
    "# Fetch samples from the sampler\n",
    "sghmc_samples = sghmc_mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9a230e",
   "metadata": {},
   "source": [
    "### Model with no positional arguments\n",
    "\n",
    "The following is a bad example where the observed data is passed as a key word argument, this will not be compatible with our implementation of SGHMC and you may get some undefined behaviour. So please pass any observed data that you wish to subsample as positional arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5160c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data=torch.zeros(10)):\n",
    "    alpha0 = torch.tensor(1.0)\n",
    "    beta0 = torch.tensor(1.0)\n",
    "    \n",
    "    f = pyro.sample(\"coin\", dist.Beta(alpha0, beta0))\n",
    "    \n",
    "    return pyro.sample(\"obs\", dist.Bernoulli(f), obs=dats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf3097",
   "metadata": {},
   "source": [
    "For example do this instead,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.zeros(10)\n",
    "\n",
    "def model(data):\n",
    "    alpha0 = torch.tensor(1.0)\n",
    "    beta0 = torch.tensor(1.0)\n",
    "    \n",
    "    f = pyro.sample(\"coin\", dist.Beta(alpha0, beta0))\n",
    "    \n",
    "    return pyro.sample(\"obs\", dist.Bernoulli(f), obs=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedf7c3d",
   "metadata": {},
   "source": [
    "### Model with multiple positional arguments to subsample\n",
    "\n",
    "The below model specifies the outcome of two independent coin flips, we may wish to pass observation data as two seperate positional arguments. Our implementation of SGHMC is flexible enough to handle this use case we just need to set `subsample_positions` appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.ones(10)\n",
    "x1[0:6] = torch.zeros(6)\n",
    "\n",
    "x2 = torch.ones(10)\n",
    "x2[0:2] = torch.zeros(2)\n",
    "\n",
    "def model(x1, x2, alpha0=1., beta0=1.):\n",
    "    alpha0 = torch.tensor(alpha0)\n",
    "    beta0 = torch.tensor(beta0)\n",
    "    \n",
    "    f1 = pyro.sample(\"coin1\", dist.Beta(alpha0, beta0))\n",
    "    f2 = pyro.sample(\"coin2\", dist.Beta(alpha0, beta0))\n",
    "    \n",
    "    return pyro.sample(\"obs1\", dist.Bernoulli(f1), obs=x1), pyro.sample(\"obs2\", dist.Bernoulli(f2), obs=x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d43362",
   "metadata": {},
   "source": [
    "We want to subsample both `x1` and `x2` and so we must set `subsample_positions=[0, 1]` given that `x1` and `x2` are positional argument `0` and `1` respectively. Otherwise, we could get some strange behaviour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b6166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SGHMC kernel\n",
    "sghmc_kernel = SGHMC(model,\n",
    "                     subsample_positions=[0, 1],\n",
    "                     batch_size=5,\n",
    "                     learning_rate=0.01,\n",
    "                     momentum_decay= 0.1,\n",
    "                     num_steps=10,\n",
    "                     resample_every_n=50,\n",
    "                     obs_info_noise=False,\n",
    "                     compute_obs_info=None)\n",
    "\n",
    "# Use pyros MCMC sampling module\n",
    "sghmc_mcmc = MCMC(sghmc_kernel, num_samples=1000)\n",
    "# Pass the model parameters when we call run\n",
    "sghmc_mcmc.run(x1, x2)\n",
    "# Fetch samples from the sampler\n",
    "sghmc_samples = sghmc_mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8688c6",
   "metadata": {},
   "source": [
    "Note that it is **essential** that `x1` and `x2` be the same size, it doesn't make sense that during our trial of two independent coin flips we forget to flip one of the coins. It is always the case that when we condition on more than one positional argument the supplied data must have the same length in the first dimension. \n",
    "\n",
    "If you need more flexibility just pack the data into tensors and make sure the first dimensions are of equal size and unpack the data in the model. Although models that require this sort of flexbility can probably be refactored anyway to obey these restrictions with no semantic changes to the program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bfcf94",
   "metadata": {},
   "source": [
    "### Silly example\n",
    "\n",
    "This silly example is just poor practice but it shows the flexbility of our SGHMC implementation. For example suppose `alpha0` is a positional argument and `beta0` is a key word argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13e5a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, alpha0, beta0=1.):\n",
    "    alpha0 = torch.tensor(alpha0)\n",
    "    beta0 = torch.tensor(beta0)\n",
    "    \n",
    "    f = pyro.sample(\"coin\", dist.Beta(alpha0, beta0))\n",
    "    \n",
    "    return pyro.sample(\"obs\", dist.Bernoulli(f), obs=dats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061eb0fb",
   "metadata": {},
   "source": [
    "Now when we run SGHMC we have to specify `alpha0` but we could just leave `beta0` as its default value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SGHMC kernel\n",
    "sghmc_kernel = SGHMC(model,\n",
    "                     subsample_positions=[0, 1],\n",
    "                     batch_size=5,\n",
    "                     learning_rate=0.01,\n",
    "                     momentum_decay= 0.1,\n",
    "                     num_steps=10,\n",
    "                     resample_every_n=50,\n",
    "                     obs_info_noise=False,\n",
    "                     compute_obs_info=None)\n",
    "\n",
    "# Use pyros MCMC sampling module\n",
    "sghmc_mcmc = MCMC(sghmc_kernel, num_samples=1000)\n",
    "# Pass the model parameters when we call run\n",
    "# alpha0 is positional argument 1 and we must specify it or we will get an error\n",
    "sghmc_mcmc.run(data, 1.)\n",
    "# We can specify beta0 too if we want but since it is a kwarg it is optional\n",
    "sghmc_mcmc.run(data, 10., beta0=10.)\n",
    "# Fetch samples from the sampler\n",
    "sghmc_samples = sghmc_mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c8e10b",
   "metadata": {},
   "source": [
    "The following instantiation of `sghmc_kernel` would be **wrong** since we don't want to subsample alpha0!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19253db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SGHMC kernel\n",
    "sghmc_kernel = SGHMC(model, subsample_positions=[0, 1] ,batch_size=5, step_size=0.1, num_steps=4, with_friction=True, do_mh_correction=False)\n",
    "# This is worng since subsample_positions=[0, 1] which says we want to subsample positional argument 1, namely alpha0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

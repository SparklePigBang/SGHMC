{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the effect of incorporating observed information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the effect on SGHMC of using observed information to estimate the noise model. We test three configurations:\n",
    "- Not using observed information.\n",
    "- Only calculating the estimate at setup time.\n",
    "- Recalculating every sample.\n",
    "- Recalculating before every step while simulating the dynamics in a sample.\n",
    "\n",
    "We record the test accuracy and time taken for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os.path\n",
    "import json\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import seaborn as sns # conda install seaborn\n",
    "import pandas as pd # ^^ this will automatically install pandas\n",
    "\n",
    "import pyro\n",
    "from pyro.infer.mcmc import MCMC\n",
    "import pyro.distributions as dist\n",
    "\n",
    "from kernel.sghmc import SGHMC\n",
    "from kernel.sgld import SGLD\n",
    "from kernel.sgd import SGD\n",
    "from kernel.sgnuts import NUTS as SGNUTS\n",
    "\n",
    "pyro.set_rng_seed(101)\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = os.path.join(\"results\", \"obs-info\")\n",
    "RESULTS_NOINFO = os.path.join(RESULTS_DIR, \"noinfo.json\")\n",
    "RESULTS_START = os.path.join(RESULTS_DIR, \"start.json\")\n",
    "RESULTS_EVERY_SAMPLE = os.path.join(RESULTS_DIR, \"every-sample.json\")\n",
    "RESULTS_EVERY_STEP = os.path.join(RESULTS_DIR, \"every-step.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple dataset wrapper class\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.data))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf72cc",
   "metadata": {},
   "source": [
    "## Hyperparams\n",
    "\n",
    "These hyperparameters were fixed during the hyperparameter search. All other hyperparameters in this notebook are the best ones we found during the hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af08685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "NUM_EPOCHS = 800\n",
    "WARMUP_EPOCHS = 50\n",
    "HIDDEN_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9339186",
   "metadata": {},
   "source": [
    "## Download MNIST and setup datasets / dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f96971",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST('./data', train=True, download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True)\n",
    "\n",
    "nvalid = 10000\n",
    "\n",
    "perm = torch.arange(len(train_dataset))\n",
    "train_idx = perm[nvalid:]\n",
    "val_idx = perm[:nvalid]\n",
    "    \n",
    "mean = 0.1307\n",
    "std = 0.3081\n",
    "\n",
    "# scale the datasets\n",
    "X_train = train_dataset.data[train_idx] / 255.0\n",
    "Y_train = train_dataset.targets[train_idx]\n",
    "\n",
    "X_val = train_dataset.data[val_idx] / 255.0\n",
    "Y_val = train_dataset.targets[val_idx]\n",
    "\n",
    "X_test = test_dataset.data / 255.0\n",
    "Y_test = test_dataset.targets\n",
    "\n",
    "# redefine the datasets\n",
    "train_dataset = Dataset(X_train, Y_train)\n",
    "val_dataset = Dataset(X_val, Y_val)\n",
    "test_dataset = Dataset(X_test, Y_test)\n",
    "\n",
    "# setup the dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25885bb0",
   "metadata": {},
   "source": [
    "## Define the Bayesian neural network  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "PyroLinear = pyro.nn.PyroModule[torch.nn.Linear]\n",
    "    \n",
    "class BNN(pyro.nn.PyroModule):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, prec=1., device='cpu'):\n",
    "        super().__init__()\n",
    "        # prec is a kwarg that should only used by SGD to set the regularization strength \n",
    "        # recall that a Guassian prior over the weights is equivalent to L2 norm regularization in the non-Bayes setting\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        # TODO add gamma priors to precision terms\n",
    "\n",
    "        self.fc1 = PyroLinear(input_size, hidden_size)\n",
    "\n",
    "        fc1_weight_loc = torch.zeros((hidden_size, input_size), device=self.device)\n",
    "        fc1_weight_scale = torch.ones((hidden_size, input_size), device=self.device) * prec\n",
    "\n",
    "        fc1_bias_loc = torch.zeros((hidden_size,), device=self.device)\n",
    "        fc1_bias_scale = torch.ones((hidden_size,), device=self.device) * prec\n",
    "\n",
    "        self.fc1.weight = pyro.nn.PyroSample(dist.Normal(fc1_weight_loc, fc1_weight_scale).to_event(2))\n",
    "        self.fc1.bias   = pyro.nn.PyroSample(dist.Normal(fc1_bias_loc, fc1_bias_scale).to_event(1))\n",
    "        \n",
    "        self.fc2 = PyroLinear(hidden_size, output_size)\n",
    "\n",
    "        fc2_weight_loc = torch.zeros((output_size, hidden_size), device=self.device)\n",
    "        fc2_weight_scale = torch.ones((output_size, hidden_size), device=self.device) * prec\n",
    "\n",
    "        fc2_bias_loc = torch.zeros((output_size,), device=self.device)\n",
    "        fc2_bias_scale = torch.ones((output_size,), device=self.device) * prec\n",
    "\n",
    "        self.fc2.weight = pyro.nn.PyroSample(dist.Normal(fc2_weight_loc, fc2_weight_scale).to_event(2))\n",
    "        self.fc2.bias   = pyro.nn.PyroSample(dist.Normal(fc2_bias_loc, fc2_bias_scale).to_event(1))\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = x.view(-1, 28*28).to(self.device)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.log_softmax(x)# output (log) softmax probabilities of each class\n",
    "\n",
    "        if y is not None:\n",
    "            y = y.to(self.device)\n",
    "        \n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", dist.Categorical(logits=x), obs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not using observed information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c5025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LR = 2e-6\n",
    "MOMENTUM_DECAY = 0.01\n",
    "RESAMPLE_EVERY_N = 0\n",
    "NUM_STEPS = 1 # fixed during hypeparameter search\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "bnn = BNN(28*28, HIDDEN_SIZE, 10, device=device).to(device)\n",
    "\n",
    "sghmc = SGHMC(bnn,\n",
    "              subsample_positions=[0, 1],\n",
    "              batch_size=BATCH_SIZE,\n",
    "              learning_rate=LR,\n",
    "              momentum_decay=MOMENTUM_DECAY,\n",
    "              num_steps=NUM_STEPS,\n",
    "              resample_every_n=RESAMPLE_EVERY_N,\n",
    "              obs_info_noise=False,\n",
    "              device=device)\n",
    "\n",
    "sghmc_mcmc = MCMC(sghmc, num_samples=len(train_dataset)//BATCH_SIZE, warmup_steps=0)\n",
    "\n",
    "noinfo_test_errs = []\n",
    "noinfo_times = []\n",
    "\n",
    "# full posterior predictive \n",
    "full_predictive = torch.FloatTensor(10000, 10)\n",
    "full_predictive.zero_()\n",
    "\n",
    "for epoch in range(1, 1+NUM_EPOCHS + WARMUP_EPOCHS):\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    sghmc_mcmc.run(X_train, Y_train)\n",
    "    \n",
    "    if epoch >= WARMUP_EPOCHS:\n",
    "        \n",
    "        sghmc_samples = sghmc_mcmc.get_samples()\n",
    "        predictive = pyro.infer.Predictive(bnn, posterior_samples=sghmc_samples)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            epoch_predictive = None\n",
    "            for x, y in val_loader:\n",
    "                prediction = predictive(x)['obs'].to(torch.int64).to(\"cpu\")\n",
    "                if epoch_predictive is None:\n",
    "                    epoch_predictive = prediction\n",
    "                else:\n",
    "                    epoch_predictive = torch.cat((epoch_predictive, prediction), dim=1)\n",
    "                    \n",
    "            for sample in epoch_predictive:\n",
    "                predictive_one_hot = F.one_hot(sample, num_classes=10)\n",
    "                full_predictive = full_predictive + predictive_one_hot\n",
    "                \n",
    "            full_y_hat = torch.argmax(full_predictive, dim=1)\n",
    "            total = Y_val.shape[0]\n",
    "            correct = int((full_y_hat == Y_val).sum())\n",
    "            \n",
    "        end = time.time()\n",
    "        \n",
    "        noinfo_test_errs.append(1.0 - correct/total)\n",
    "        noinfo_times.append(end - start)\n",
    "\n",
    "        print(\"Epoch [{}/{}] test accuracy: {:.4f} time: {:.2f}\".format(epoch-WARMUP_EPOCHS, NUM_EPOCHS, correct/total, end - start))\n",
    "\n",
    "# Save the errors to a file\n",
    "with open(RESULTS_NOINFO, \"w\") as f:\n",
    "    json.dump((noinfo_test_errs, noinfo_times), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating at setup time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c5025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LR = 2e-6\n",
    "MOMENTUM_DECAY = 0.01\n",
    "RESAMPLE_EVERY_N = 0\n",
    "NUM_STEPS = 1 # fixed during hypeparameter search\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "bnn = BNN(28*28, HIDDEN_SIZE, 10, device=device).to(device)\n",
    "\n",
    "sghmc = SGHMC(bnn,\n",
    "              subsample_positions=[0, 1],\n",
    "              batch_size=BATCH_SIZE,\n",
    "              learning_rate=LR,\n",
    "              momentum_decay=MOMENTUM_DECAY,\n",
    "              num_steps=NUM_STEPS,\n",
    "              resample_every_n=RESAMPLE_EVERY_N,\n",
    "              obs_info_noise=True,\n",
    "              compute_obs_info=\"start\",\n",
    "              device=device)\n",
    "\n",
    "sghmc_mcmc = MCMC(sghmc, num_samples=len(train_dataset)//BATCH_SIZE, warmup_steps=0)\n",
    "\n",
    "start_test_errs = []\n",
    "start_times = []\n",
    "\n",
    "# full posterior predictive \n",
    "full_predictive = torch.FloatTensor(10000, 10)\n",
    "full_predictive.zero_()\n",
    "\n",
    "for epoch in range(1, 1+NUM_EPOCHS + WARMUP_EPOCHS):\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    sghmc_mcmc.run(X_train, Y_train)\n",
    "    \n",
    "    if epoch >= WARMUP_EPOCHS:\n",
    "        \n",
    "        sghmc_samples = sghmc_mcmc.get_samples()\n",
    "        predictive = pyro.infer.Predictive(bnn, posterior_samples=sghmc_samples)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            epoch_predictive = None\n",
    "            for x, y in val_loader:\n",
    "                prediction = predictive(x)['obs'].to(torch.int64).to(\"cpu\")\n",
    "                if epoch_predictive is None:\n",
    "                    epoch_predictive = prediction\n",
    "                else:\n",
    "                    epoch_predictive = torch.cat((epoch_predictive, prediction), dim=1)\n",
    "                    \n",
    "            for sample in epoch_predictive:\n",
    "                predictive_one_hot = F.one_hot(sample, num_classes=10)\n",
    "                full_predictive = full_predictive + predictive_one_hot\n",
    "                \n",
    "            full_y_hat = torch.argmax(full_predictive, dim=1)\n",
    "            total = Y_val.shape[0]\n",
    "            correct = int((full_y_hat == Y_val).sum())\n",
    "            \n",
    "        end = time.time()\n",
    "        \n",
    "        start_test_errs.append(1.0 - correct/total)\n",
    "        start_times.append(end - start)\n",
    "\n",
    "        print(\"Epoch [{}/{}] test accuracy: {:.4f} time: {:.2f}\".format(epoch-WARMUP_EPOCHS, NUM_EPOCHS, correct/total, end - start))\n",
    "\n",
    "# Save the errors to a file\n",
    "with open(RESULTS_START, \"w\") as f:\n",
    "    json.dump((start_test_errs, start_times), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recalculating every sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c5025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LR = 2e-6\n",
    "MOMENTUM_DECAY = 0.01\n",
    "RESAMPLE_EVERY_N = 0\n",
    "NUM_STEPS = 1 # fixed during hypeparameter search\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "bnn = BNN(28*28, HIDDEN_SIZE, 10, device=device).to(device)\n",
    "\n",
    "sghmc = SGHMC(bnn,\n",
    "              subsample_positions=[0, 1],\n",
    "              batch_size=BATCH_SIZE,\n",
    "              learning_rate=LR,\n",
    "              momentum_decay=MOMENTUM_DECAY,\n",
    "              num_steps=NUM_STEPS,\n",
    "              resample_every_n=RESAMPLE_EVERY_N,\n",
    "              obs_info_noise=True,\n",
    "              compute_obs_info=\"every_sample\",\n",
    "              device=device)\n",
    "\n",
    "sghmc_mcmc = MCMC(sghmc, num_samples=len(train_dataset)//BATCH_SIZE, warmup_steps=0)\n",
    "\n",
    "every_sample_test_errs = []\n",
    "every_sample_times = []\n",
    "\n",
    "# full posterior predictive \n",
    "full_predictive = torch.FloatTensor(10000, 10)\n",
    "full_predictive.zero_()\n",
    "\n",
    "for epoch in range(1, 1+NUM_EPOCHS + WARMUP_EPOCHS):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    sghmc_mcmc.run(X_train, Y_train)\n",
    "    \n",
    "    if epoch >= WARMUP_EPOCHS:\n",
    "        \n",
    "        sghmc_samples = sghmc_mcmc.get_samples()\n",
    "        predictive = pyro.infer.Predictive(bnn, posterior_samples=sghmc_samples)\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            epoch_predictive = None\n",
    "            for x, y in val_loader:\n",
    "                prediction = predictive(x)['obs'].to(torch.int64).to(\"cpu\")\n",
    "                if epoch_predictive is None:\n",
    "                    epoch_predictive = prediction\n",
    "                else:\n",
    "                    epoch_predictive = torch.cat((epoch_predictive, prediction), dim=1)\n",
    "                    \n",
    "            for sample in epoch_predictive:\n",
    "                predictive_one_hot = F.one_hot(sample, num_classes=10)\n",
    "                full_predictive = full_predictive + predictive_one_hot\n",
    "                \n",
    "            full_y_hat = torch.argmax(full_predictive, dim=1)\n",
    "            total = Y_val.shape[0]\n",
    "            correct = int((full_y_hat == Y_val).sum())\n",
    "            \n",
    "        end = time.time()\n",
    "        \n",
    "        every_sample_test_errs.append(1.0 - correct/total)\n",
    "        every_sample_times.append(end - start)\n",
    "\n",
    "        print(\"Epoch [{}/{}] test accuracy: {:.4f} time: {:.2f}\".format(epoch-WARMUP_EPOCHS, NUM_EPOCHS, correct/total, end - start))\n",
    "\n",
    "# Save the errors to a file\n",
    "with open(RESULTS_EVERY_SAMPLE, \"w\") as f:\n",
    "    json.dump((every_sample_test_errs, every_sample_times), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recalculating every step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c5025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LR = 2e-6\n",
    "MOMENTUM_DECAY = 0.01\n",
    "RESAMPLE_EVERY_N = 0\n",
    "NUM_STEPS = 1 # fixed during hypeparameter search\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "bnn = BNN(28*28, HIDDEN_SIZE, 10, device=device).to(device)\n",
    "\n",
    "sghmc = SGHMC(bnn,\n",
    "              subsample_positions=[0, 1],\n",
    "              batch_size=BATCH_SIZE,\n",
    "              learning_rate=LR,\n",
    "              momentum_decay=MOMENTUM_DECAY,\n",
    "              num_steps=NUM_STEPS,\n",
    "              resample_every_n=RESAMPLE_EVERY_N,\n",
    "              obs_info_noise=True,\n",
    "              compute_obs_info=\"every_step\",\n",
    "              device=device)\n",
    "\n",
    "sghmc_mcmc = MCMC(sghmc, num_samples=len(train_dataset)//BATCH_SIZE, warmup_steps=0)\n",
    "\n",
    "every_step_test_errs = []\n",
    "every_step_times = []\n",
    "\n",
    "# full posterior predictive \n",
    "full_predictive = torch.FloatTensor(10000, 10)\n",
    "full_predictive.zero_()\n",
    "\n",
    "for epoch in range(1, 1+NUM_EPOCHS + WARMUP_EPOCHS):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    sghmc_mcmc.run(X_train, Y_train)\n",
    "    \n",
    "    if epoch >= WARMUP_EPOCHS:\n",
    "        \n",
    "        sghmc_samples = sghmc_mcmc.get_samples()\n",
    "        predictive = pyro.infer.Predictive(bnn, posterior_samples=sghmc_samples)\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            epoch_predictive = None\n",
    "            for x, y in val_loader:\n",
    "                prediction = predictive(x)['obs'].to(torch.int64).to(\"cpu\")\n",
    "                if epoch_predictive is None:\n",
    "                    epoch_predictive = prediction\n",
    "                else:\n",
    "                    epoch_predictive = torch.cat((epoch_predictive, prediction), dim=1)\n",
    "                    \n",
    "            for sample in epoch_predictive:\n",
    "                predictive_one_hot = F.one_hot(sample, num_classes=10)\n",
    "                full_predictive = full_predictive + predictive_one_hot\n",
    "                \n",
    "            full_y_hat = torch.argmax(full_predictive, dim=1)\n",
    "            total = Y_val.shape[0]\n",
    "            correct = int((full_y_hat == Y_val).sum())\n",
    "            \n",
    "        end = time.time()\n",
    "        \n",
    "        every_step_test_errs.append(1.0 - correct/total)\n",
    "        every_step_times.append(end - start)\n",
    "\n",
    "        print(\"Epoch [{}/{}] test accuracy: {:.4f} time: {:.2f}\".format(epoch-WARMUP_EPOCHS, NUM_EPOCHS, correct/total, end - start))\n",
    "\n",
    "# Save the errors to a file\n",
    "with open(RESULTS_EVERY_STEP, \"w\") as f:\n",
    "    json.dump((every_step_test_errs, every_step_times), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a3e7f",
   "metadata": {},
   "source": [
    "### Plot the convergence dynamics and times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aed042",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"dark\")\n",
    "\n",
    "# Load the previous results from the files\n",
    "with open(RESULTS_NOINFO, \"r\") as f:\n",
    "    noinfo_test_errs, noinfo_times = json.load(f)\n",
    "with open(RESULTS_START, \"r\") as f:\n",
    "    start_test_errs, start_times = json.load(f)\n",
    "with open(RESULTS_EVERY_SAMPLE, \"r\") as f:\n",
    "    every_sample_test_errs, every_sample_times = json.load(f)\n",
    "with open(RESULTS_EVERY_STEP, \"r\") as f:\n",
    "    every_step_test_errs, every_step_times = json.load(f)\n",
    "    \n",
    "noinfo_test_errs = np.array(noinfo_test_errs)\n",
    "noinfo_times = np.array(noinfo_times)\n",
    "start_test_errs = np.array(start_test_errs)\n",
    "start_times = np.array(start_times)\n",
    "every_sample_test_errs = np.array(every_sample_test_errs)\n",
    "every_sample_times = np.array(every_sample_times)\n",
    "every_step_test_errs = np.array(every_step_test_errs)\n",
    "every_step_times = np.array(every_step_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_dict = {\n",
    "    'No obs info' : noinfo_test_errs, \n",
    "    'Start' : start_test_errs, \n",
    "    'Every sample' : every_sample_test_errs, \n",
    "    'Every step' : every_step_test_errs\n",
    "}\n",
    "x = np.arange(1, NUM_EPOCHS+1)\n",
    "lst = []\n",
    "for i in range(len(x)):\n",
    "    for updater in err_dict.keys():\n",
    "        lst.append([x[i], updater, err_dict[updater][i]])\n",
    "\n",
    "df = pd.DataFrame(lst, columns=['iterations', 'updater','test error'])\n",
    "sns.lineplot(data=df.pivot(\"iterations\", \"updater\", \"test error\"))\n",
    "plt.ylabel(\"test error\")\n",
    "plt.show() #dpi=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dict = {\n",
    "    'No obs info' : noinfo_times, \n",
    "    'Start' : start_times, \n",
    "    'Every sample' : every_sample_times, \n",
    "    'Every step' : every_step_times\n",
    "}\n",
    "x = np.arange(1, NUM_EPOCHS+1)\n",
    "lst = []\n",
    "for i in range(len(x)):\n",
    "    for updater in err_dict.keys():\n",
    "        lst.append([x[i], updater, time_dict[updater][i]])\n",
    "\n",
    "df = pd.DataFrame(lst, columns=['iterations', 'updater','sample time'])\n",
    "sns.lineplot(data=df.pivot(\"iterations\", \"updater\", \"sample time\"))\n",
    "plt.ylabel(\"sample time\")\n",
    "plt.show() #dpi=300"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac8f97efb1f820d1578ffdeb62b1dbf00f6dafb62ed9a2204fceaad3b0770e21"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('atml-course')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "058c20a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os.path\n",
    "import json\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import seaborn as sns # conda install seaborn\n",
    "import pandas as pd # ^^ this will automatically install pandas\n",
    "\n",
    "import pyro\n",
    "from pyro.infer.mcmc import MCMC\n",
    "import pyro.distributions as dist\n",
    "\n",
    "from kernel.sghmc import SGHMC\n",
    "from kernel.sgld import SGLD\n",
    "from kernel.sgd import SGD\n",
    "from kernel.sgnuts import NUTS as SGNUTS\n",
    "\n",
    "pyro.set_rng_seed(101)\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1984ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f1b809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = os.path.join(\"results\", \"bnn\")\n",
    "RESULTS_SGHMC = os.path.join(RESULTS_DIR, \"sghmc.json\")\n",
    "RESULTS_SGLD = os.path.join(RESULTS_DIR, \"sgld.json\")\n",
    "RESULTS_SGD = os.path.join(RESULTS_DIR, \"sgd.json\")\n",
    "RESULTS_SGDMOM = os.path.join(RESULTS_DIR, \"sgdmom.json\")\n",
    "\n",
    "# Save the errors to a file\n",
    "with open(RESULTS_SGHMC, \"w\") as f:\n",
    "    json.dump([0, 4, 3], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b6cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple dataset wrapper class\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.data))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf72cc",
   "metadata": {},
   "source": [
    "### Hyperparams\n",
    "\n",
    "These hyperparameters were fixed during the hyperparameter search. All other hyperparameters in this notebook are the best ones we found during the hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af08685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "NUM_EPOCHS = 800\n",
    "WARMUP_EPOCHS = 50\n",
    "HIDDEN_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9339186",
   "metadata": {},
   "source": [
    "### Download MNIST and setup datasets / dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f96971",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST('./data', train=True, download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True)\n",
    "\n",
    "nvalid = 10000\n",
    "\n",
    "perm = torch.arange(len(train_dataset))\n",
    "train_idx = perm[nvalid:]\n",
    "val_idx = perm[:nvalid]\n",
    "    \n",
    "mean = 0.1307\n",
    "std = 0.3081\n",
    "\n",
    "# scale the datasets\n",
    "X_train = train_dataset.data[train_idx] / 255.0\n",
    "Y_train = train_dataset.targets[train_idx]\n",
    "\n",
    "X_val = train_dataset.data[val_idx] / 255.0\n",
    "Y_val = train_dataset.targets[val_idx]\n",
    "\n",
    "X_test = test_dataset.data / 255.0\n",
    "Y_test = test_dataset.targets\n",
    "\n",
    "# redefine the datasets\n",
    "train_dataset = Dataset(X_train, Y_train)\n",
    "val_dataset = Dataset(X_val, Y_val)\n",
    "test_dataset = Dataset(X_test, Y_test)\n",
    "\n",
    "# setup the dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25885bb0",
   "metadata": {},
   "source": [
    "### Define the Bayesian neural network  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "PyroLinear = pyro.nn.PyroModule[torch.nn.Linear]\n",
    "    \n",
    "class BNN(pyro.nn.PyroModule):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, prec=1., device='cpu'):\n",
    "        super().__init__()\n",
    "        # prec is a kwarg that should only used by SGD to set the regularization strength \n",
    "        # recall that a Guassian prior over the weights is equivalent to L2 norm regularization in the non-Bayes setting\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        # TODO add gamma priors to precision terms\n",
    "\n",
    "        self.fc1 = PyroLinear(input_size, hidden_size)\n",
    "\n",
    "        fc1_weight_loc = torch.zeros((hidden_size, input_size), device=self.device)\n",
    "        fc1_weight_scale = torch.ones((hidden_size, input_size), device=self.device) * prec\n",
    "\n",
    "        fc1_bias_loc = torch.zeros((hidden_size,), device=self.device)\n",
    "        fc1_bias_scale = torch.ones((hidden_size,), device=self.device) * prec\n",
    "\n",
    "        self.fc1.weight = pyro.nn.PyroSample(dist.Normal(fc1_weight_loc, fc1_weight_scale).to_event(2))\n",
    "        self.fc1.bias   = pyro.nn.PyroSample(dist.Normal(fc1_bias_loc, fc1_bias_scale).to_event(1))\n",
    "        \n",
    "        self.fc2 = PyroLinear(hidden_size, output_size)\n",
    "\n",
    "        fc2_weight_loc = torch.zeros((output_size, hidden_size), device=self.device)\n",
    "        fc2_weight_scale = torch.ones((output_size, hidden_size), device=self.device) * prec\n",
    "\n",
    "        fc2_bias_loc = torch.zeros((output_size,), device=self.device)\n",
    "        fc2_bias_scale = torch.ones((output_size,), device=self.device) * prec\n",
    "\n",
    "        self.fc2.weight = pyro.nn.PyroSample(dist.Normal(fc2_weight_loc, fc2_weight_scale).to_event(2))\n",
    "        self.fc2.bias   = pyro.nn.PyroSample(dist.Normal(fc2_bias_loc, fc2_bias_scale).to_event(1))\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = x.view(-1, 28*28).to(self.device)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.log_softmax(x)# output (log) softmax probabilities of each class\n",
    "\n",
    "        if y is not None:\n",
    "            y = y.to(self.device)\n",
    "        \n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", dist.Categorical(logits=x), obs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06b5534",
   "metadata": {},
   "source": [
    "### Run SGHMC \n",
    "\n",
    "We run SGHMC to sample approximately from the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c5025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LR = 2e-6\n",
    "MOMENTUM_DECAY = 0.01\n",
    "RESAMPLE_EVERY_N = 0\n",
    "NUM_STEPS = 1 # fixed during hypeparameter search\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "bnn = BNN(28*28, HIDDEN_SIZE, 10, device=device).to(device)\n",
    "\n",
    "sghmc = SGHMC(bnn,\n",
    "              subsample_positions=[0, 1],\n",
    "              batch_size=BATCH_SIZE,\n",
    "              learning_rate=LR,\n",
    "              momentum_decay=MOMENTUM_DECAY,\n",
    "              num_steps=NUM_STEPS,\n",
    "              resample_every_n=RESAMPLE_EVERY_N,\n",
    "              obs_info_noise=True,\n",
    "              device=device)\n",
    "\n",
    "sghmc_mcmc = MCMC(sghmc, num_samples=len(train_dataset)//BATCH_SIZE, warmup_steps=0)\n",
    "\n",
    "sghmc_test_errs = []\n",
    "\n",
    "# full posterior predictive \n",
    "full_predictive = torch.FloatTensor(10000, 10)\n",
    "full_predictive.zero_()\n",
    "\n",
    "for epoch in range(1, 1+NUM_EPOCHS + WARMUP_EPOCHS):\n",
    "    sghmc_mcmc.run(X_train, Y_train)\n",
    "    \n",
    "    if epoch >= WARMUP_EPOCHS:\n",
    "        \n",
    "        sghmc_samples = sghmc_mcmc.get_samples()\n",
    "        predictive = pyro.infer.Predictive(bnn, posterior_samples=sghmc_samples)\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            epoch_predictive = None\n",
    "            for x, y in val_loader:\n",
    "                prediction = predictive(x)['obs'].to(torch.int64).to(\"cpu\")\n",
    "                if epoch_predictive is None:\n",
    "                    epoch_predictive = prediction\n",
    "                else:\n",
    "                    epoch_predictive = torch.cat((epoch_predictive, prediction), dim=1)\n",
    "                    \n",
    "            for sample in epoch_predictive:\n",
    "                predictive_one_hot = F.one_hot(sample, num_classes=10)\n",
    "                full_predictive = full_predictive + predictive_one_hot\n",
    "                \n",
    "            full_y_hat = torch.argmax(full_predictive, dim=1)\n",
    "            total = Y_val.shape[0]\n",
    "            correct = int((full_y_hat == Y_val).sum())\n",
    "            \n",
    "        end = time.time()\n",
    "        \n",
    "        sghmc_test_errs.append(1.0 - correct/total)\n",
    "\n",
    "        print(\"Epoch [{}/{}] test accuracy: {:.4f} time: {:.2f}\".format(epoch-WARMUP_EPOCHS, NUM_EPOCHS, correct/total, end - start))\n",
    "        \n",
    "        # Epoch [800/800] test accuracy: 0.9714 time: 4.26\n",
    "\n",
    "# Save the errors to a file\n",
    "with open(RESULTS_SGHMC, \"w\") as f:\n",
    "    json.dump(sghmc_test_errs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8828292c",
   "metadata": {},
   "source": [
    "### Run SGLD\n",
    "\n",
    "We run SGLD to sample approximately from the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902568b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 4e-5\n",
    "LR_DECAY = False\n",
    "\n",
    "if LR_DECAY:\n",
    "    D = 0.25 # decay by 1/4\n",
    "    B = (NUM_EPOCHS * D**2) / (1 - D**2)\n",
    "    A = LR * np.sqrt((NUM_EPOCHS * D**2) / (1 - D**2))\n",
    "    \n",
    "NUM_STEPS = 1\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "bnn = BNN(28*28, HIDDEN_SIZE, 10)\n",
    "\n",
    "sgld = SGLD(bnn,\n",
    "            subsample_positions=[0, 1],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            learning_rate=LR,\n",
    "            num_steps=NUM_STEPS)\n",
    "\n",
    "sgld_mcmc = MCMC(sgld, num_samples=len(train_dataset)//BATCH_SIZE, warmup_steps=0)\n",
    "\n",
    "sgld_test_errs = []\n",
    "\n",
    "# full posterior predictive \n",
    "full_predictive = torch.FloatTensor(10000, 10)\n",
    "full_predictive.zero_()\n",
    "\n",
    "for epoch in range(1, 1+NUM_EPOCHS + WARMUP_EPOCHS):   \n",
    "    sgld_mcmc.run(X_train, Y_train)\n",
    "    \n",
    "    if epoch >= WARMUP_EPOCHS:\n",
    "        if LR_DECAY:\n",
    "            LR = A / np.sqrt((B + (epoch-1)))\n",
    "            sgld_mcmc.kernel.learning_rate = LR\n",
    "        \n",
    "        start = time.time()\n",
    "        sgld_samples = sgld_mcmc.get_samples()\n",
    "        predictive = pyro.infer.Predictive(bnn, posterior_samples=sgld_samples)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            epoch_predictive = None\n",
    "            for x, y in val_loader:\n",
    "                if epoch_predictive is None:\n",
    "                    epoch_predictive = predictive(x)['obs'].to(torch.int64)\n",
    "                else:\n",
    "                    epoch_predictive = torch.cat((epoch_predictive, predictive(x)['obs'].to(torch.int64)), dim=1)\n",
    "            \n",
    "            for sample in epoch_predictive:\n",
    "                predictive_one_hot = F.one_hot(sample, num_classes=10)\n",
    "                if LR_DECAY:\n",
    "                    predictive_one_hot = predictive_one_hot * LR\n",
    "                full_predictive = full_predictive + predictive_one_hot\n",
    "                \n",
    "            full_y_hat = torch.argmax(full_predictive, dim=1)\n",
    "            total = Y_val.shape[0]\n",
    "            correct = int((full_y_hat == Y_val).sum())\n",
    "            \n",
    "        end = time.time()\n",
    "        \n",
    "        sgld_test_errs.append(1.0 - correct/total)\n",
    "\n",
    "        print(\"Epoch [{}/{}] test accuracy: {:.4f} time: {:.2f}\".format(epoch-WARMUP_EPOCHS, NUM_EPOCHS, correct/total, end - start))\n",
    "        \n",
    "        # Epoch [800/800] test accuracy: 0.9563 time: 4.31\n",
    "\n",
    "# Save the errors to a file\n",
    "with open(RESULTS_SGLD, \"w\") as f:\n",
    "    json.dump(sgld_test_errs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642fac32",
   "metadata": {},
   "source": [
    "### Run SGD\n",
    "\n",
    "We run SGD to optimise the weights of the BNN and we take a point estimate which is the most recent sample to be our \"best\" parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51d2987",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-5\n",
    "WEIGHT_DECAY=0.0\n",
    "WITH_MOMENTUM=False\n",
    "REGULARIZATION_TERM=1.\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "bnn = BNN(28*28, HIDDEN_SIZE, 10, prec=REGULARIZATION_TERM)\n",
    "\n",
    "sgd = SGD(bnn,\n",
    "          subsample_positions=[0, 1],\n",
    "          batch_size=BATCH_SIZE,\n",
    "          learning_rate=LR,\n",
    "          weight_decay=WEIGHT_DECAY,\n",
    "          with_momentum=WITH_MOMENTUM)\n",
    "\n",
    "sgd_mcmc = MCMC(sgd, num_samples=len(train_dataset)//BATCH_SIZE, warmup_steps=0)\n",
    "\n",
    "sgd_test_errs = []\n",
    "\n",
    "for epoch in range(1, 1+NUM_EPOCHS+WARMUP_EPOCHS):\n",
    "    sgd_mcmc.run(X_train, Y_train)\n",
    "        \n",
    "    if epoch >= WARMUP_EPOCHS:\n",
    "        \n",
    "        sgd_samples = sgd_mcmc.get_samples()\n",
    "        point_estimate = {site : sgd_samples[site][-1, :].unsqueeze(0) for site in sgd_samples.keys()}\n",
    "        predictive = pyro.infer.Predictive(bnn, posterior_samples=point_estimate)\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for x, y in val_loader:\n",
    "                batch_predictive = predictive(x)['obs']\n",
    "                batch_y_hat = batch_predictive.mode(0)[0]\n",
    "                total += y.shape[0]\n",
    "                correct += int((batch_y_hat == y).sum())\n",
    "            \n",
    "        end = time.time()\n",
    "        \n",
    "        sgd_test_errs.append(1.0 - correct/total)\n",
    "\n",
    "        print(\"Epoch [{}/{}] test accuracy: {:.4f} time: {:.2f}\".format(epoch-WARMUP_EPOCHS, NUM_EPOCHS, correct/total, end - start))\n",
    "        \n",
    "        # Epoch [800/800] test accuracy: 0.9565 time: 0.34\n",
    "\n",
    "# Save the errors to a file\n",
    "with open(RESULTS_SGD, \"w\") as f:\n",
    "    json.dump(sgd_test_errs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053562e4",
   "metadata": {},
   "source": [
    "### Run SGD with momentum\n",
    "\n",
    "We run SGD with momentum to optimise the weights of the BNN and we take a point estimate which is the most recent sample to be our \"best\" parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8737fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-6\n",
    "WEIGHT_DECAY=0.0\n",
    "WITH_MOMENTUM=True\n",
    "MOMENTUM_DECAY=0.01\n",
    "REGULARIZATION_TERM=1.\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "bnn = BNN(28*28, HIDDEN_SIZE, 10, prec=REGULARIZATION_TERM)\n",
    "\n",
    "sgdmom = SGD(bnn,\n",
    "             subsample_positions=[0, 1],\n",
    "             batch_size=BATCH_SIZE,\n",
    "             learning_rate=LR,\n",
    "             weight_decay=WEIGHT_DECAY,\n",
    "             with_momentum=WITH_MOMENTUM,\n",
    "             momentum_decay=MOMENTUM_DECAY)\n",
    "\n",
    "sgdmom_mcmc = MCMC(sgdmom, num_samples=len(train_dataset)//BATCH_SIZE, warmup_steps=0)\n",
    "\n",
    "sgdmom_test_errs = []\n",
    "\n",
    "for epoch in range(1, 1+NUM_EPOCHS+WARMUP_EPOCHS):\n",
    "    sgdmom_mcmc.run(X_train, Y_train)\n",
    "        \n",
    "    if epoch >= WARMUP_EPOCHS:\n",
    "        \n",
    "        sgdmom_samples = sgdmom_mcmc.get_samples()\n",
    "        point_estimate = {site : sgdmom_samples[site][-1, :].unsqueeze(0) for site in sgdmom_samples.keys()}\n",
    "        predictive = pyro.infer.Predictive(bnn, posterior_samples=point_estimate)\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for x, y in val_loader:\n",
    "                batch_predictive = predictive(x)['obs']\n",
    "                batch_y_hat = batch_predictive.mode(0)[0]\n",
    "                total += y.shape[0]\n",
    "                correct += int((batch_y_hat == y).sum())\n",
    "            \n",
    "        end = time.time()\n",
    "        \n",
    "        sgdmom_test_errs.append(1.0 - correct/total)\n",
    "\n",
    "        print(\"Epoch [{}/{}] test accuracy: {:.4f} time: {:.2f}\".format(epoch-WARMUP_EPOCHS, NUM_EPOCHS, correct/total, end - start))\n",
    "        \n",
    "        # Epoch [800/800] test accuracy: 0.9663 time: 0.26\n",
    "\n",
    "# Save the errors to a file\n",
    "with open(RESULTS_SGDMOM, \"w\") as f:\n",
    "    json.dump(sgdmom_test_errs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a3e7f",
   "metadata": {},
   "source": [
    "### Plot the convergence dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aed042",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"dark\")\n",
    "\n",
    "# Load the previous results from the files\n",
    "with open(RESULTS_SGHMC, \"r\") as f:\n",
    "    sghmc_test_errs = json.load(f)\n",
    "with open(RESULTS_SGLD, \"r\") as f:\n",
    "    sgld_test_errs = json.load(f)\n",
    "with open(RESULTS_SGD, \"r\") as f:\n",
    "    sgd_test_errs = json.load(f)\n",
    "with open(RESULTS_SGDMOM, \"r\") as f:\n",
    "    sgdmom_test_errs = json.load(f)\n",
    "    \n",
    "sghmc_test_errs = np.array(sghmc_test_errs)\n",
    "sgld_test_errs = np.array(sgld_test_errs)\n",
    "sgd_test_errs = np.array(sgd_test_errs)\n",
    "sgdmom_test_errs = np.array(sgdmom_test_errs)\n",
    "\n",
    "err_dict = {'SGHMC' : sghmc_test_errs, 'SGLD' : sgld_test_errs, 'SGD' : sgd_test_errs, 'SGD with momentum' : sgdmom_test_errs}\n",
    "x = np.arange(1, NUM_EPOCHS+1)\n",
    "lst = []\n",
    "for i in range(len(x)):\n",
    "    for updater in err_dict.keys():\n",
    "        lst.append([x[i], updater, err_dict[updater][i]])\n",
    "\n",
    "df = pd.DataFrame(lst, columns=['iterations', 'updater','test error'])\n",
    "sns.lineplot(data=df.pivot(\"iterations\", \"updater\", \"test error\"))\n",
    "plt.ylabel(\"test error\")\n",
    "plt.show() #dpi=300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e0df1",
   "metadata": {},
   "source": [
    "### Stochastic Gradient NUTS \n",
    "*experimental doesn't quite work yet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f463b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 2e-6\n",
    "MOMENTUM_DECAY = 0.01\n",
    "RESAMPLE_EVERY_N = 0\n",
    "NUM_STEPS = 1\n",
    "\n",
    "WARMUP_EPOCHS = 5 \n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "bnn = BNN(28*28, HIDDEN_SIZE, 10)\n",
    "\n",
    "sgnuts = SGNUTS(bnn, \n",
    "                subsample_positions=[0, 1],\n",
    "                batch_size=BATCH_SIZE,\n",
    "                learning_rate=LR, \n",
    "                momentum_decay=MOMENTUM_DECAY,\n",
    "                resample_every_n=RESAMPLE_EVERY_N, \n",
    "                obs_info_noise=False, \n",
    "                use_multinomial_sampling=True,\n",
    "                max_tree_depth=10)\n",
    "\n",
    "# do warm up\n",
    "sgnuts_mcmc = MCMC(sgnuts, num_samples=len(train_dataset)//BATCH_SIZE, warmup_steps=0)\n",
    "\n",
    "# full posterior predictive \n",
    "full_predictive = torch.FloatTensor(10000, 10)\n",
    "full_predictive.zero_()\n",
    "\n",
    "for epoch in range(1, 1+NUM_EPOCHS + WARMUP_EPOCHS):\n",
    "    sgnuts_mcmc.run(X_train, Y_train)\n",
    "    \n",
    "    if epoch >= WARMUP_EPOCHS:\n",
    "        \n",
    "        sgnuts_samples = sgnuts_mcmc.get_samples()\n",
    "        predictive = pyro.infer.Predictive(bnn, posterior_samples=sgnuts_samples)\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            epoch_predictive = None\n",
    "            for x, y in val_loader:\n",
    "                if epoch_predictive is None:\n",
    "                    epoch_predictive = predictive(x)['obs'].to(torch.int64)\n",
    "                else:\n",
    "                    epoch_predictive = torch.cat((epoch_predictive, predictive(x)['obs'].to(torch.int64)), dim=1)\n",
    "                    \n",
    "            for sample in epoch_predictive:\n",
    "                predictive_one_hot = F.one_hot(sample, num_classes=10)\n",
    "                full_predictive = full_predictive + predictive_one_hot\n",
    "                \n",
    "            full_y_hat = torch.argmax(full_predictive, dim=1)\n",
    "            total = Y_val.shape[0]\n",
    "            correct = int((full_y_hat == Y_val).sum())\n",
    "            \n",
    "        end = time.time()\n",
    "\n",
    "        print(\"Epoch [{}/{}] test accuracy: {:.4f} time: {:.2f}\".format(epoch-WARMUP_EPOCHS, NUM_EPOCH, correct/total, end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f55bd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
